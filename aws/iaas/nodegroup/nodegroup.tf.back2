variable nodegroup {}
variable vpc {}
variable eks {}
variable subnets {}
variable aws_credentials {}
variable cluster_name{}

locals {
    eks_subnets = compact([for key, subnet in var.subnets : subnet.is_eks_subnet == true ? key : ""])
}

data "aws_subnets" "eks_subnets" {
    filter {
        name   = "vpc-id"
        values = [var.vpc.vpc_id]
    }
    filter {
        name = "tag:Name"
        values = local.eks_subnets
    }
}

/*data "aws_security_groups" "select_cluster" {
    filter {
        name   = "vpc-id"
        values = [var.vpc.vpc_id]
    }
    filter {
        name = "aws:eks:cluster-name"
        values = [var.cluster_name]
    }
}*/

resource "aws_eks_node_group" "eks_ng" {
  /* depends_on = [
      aws_iam_role_policy_attachment.ng_cni_policy,
      aws_iam_role_policy_attachment.ng_lb_full_access_policy,
      aws_iam_role_policy_attachment.ng_registry_policy,
      aws_iam_role_policy_attachment.ng_s3_full_access_policy,
      aws_iam_role_policy_attachment.ng_worker_policy
    ]*/

    for_each        = var.nodegroup
    cluster_name    = var.cluster_name
    name = each.key
    //node_role_arn   = aws_iam_role.eks_ng_role.arn
    vpc_id          = var.vpc.vpc_id
    
    subnet_ids      = data.aws_subnets.eks_subnets.ids

    #cluster_primary_security_group_id = data.aws_security_groups.select_cluster.ids[0]
    #cluster_security_group_id         = var.eks.node_security_group_id
    //scaling_config {
        desired_size      = each.value["pool_size"]
        max_size          = each.value["max_size"]
        min_size          = each.value["min_size"]
    //}

    instance_types    = each.value["machine_type"]
    disk_size         = each.value["disk_size"]
    ami_type          = each.value["ami_type"]
    labels            = each.value["node_labels"]
    tags              = each.value["node_tags"]

}

resource "null_resource" "node_labels" {
    depends_on = [aws_eks_node_group.eks_ng]
    for_each = var.nodegroup


    provisioner "local-exec" {
        command = <<EOT
        export AWS_ACCESS_KEY_ID=${var.aws_credentials.aws_access_key}
        export AWS_SECRET_ACCESS_KEY=${var.aws_credentials.aws_secret_key}
        export AWS_SESSION_TOKEN=${var.aws_credentials.aws_session_token}

        aws eks update-kubeconfig --name ${var.cluster_name}
        #kubectl label node -l role="${each.value["node_role"]}" node-role.kubernetes.io/${each.value["node_role"]}="${each.value["node_role"]}" --overwrite
        EOT
    }
}
/*
resource "aws_iam_role" "eks_ng_role" {
  name = "${var.cluster_name}-eks-ng-role"

  assume_role_policy = <<POLICY
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "ec2.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
POLICY
}


resource "aws_iam_role_policy_attachment" "ng_worker_policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
  role       = aws_iam_role.eks_ng_role.name
}

resource "aws_iam_role_policy_attachment" "ng_cni_policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
  role       = aws_iam_role.eks_ng_role.name
}

resource "aws_iam_role_policy_attachment" "ng_registry_policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
  role       = aws_iam_role.eks_ng_role.name
}

resource "aws_iam_role_policy_attachment" "ng_lb_full_access_policy" {
    policy_arn = "arn:aws:iam::aws:policy/ElasticLoadBalancingFullAccess"
    role       = aws_iam_role.eks_ng_role.name    
}

resource "aws_iam_role_policy_attachment" "ng_s3_full_access_policy" {
    policy_arn = "arn:aws:iam::aws:policy/AmazonS3FullAccess"
    role       = aws_iam_role.eks_ng_role.name    
}

*/